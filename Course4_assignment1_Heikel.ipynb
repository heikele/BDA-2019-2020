{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE:\n",
    "I was having issues with timing, even on the CSC servers so I often had to set a timeout limit. It is possible that for the LSH \n",
    "I used fnames[:n] where n is how many documents used. I need to review this code and see why there were issues with LSH otherwise when running all of the abstracts through it. \n",
    "\n",
    "I was also working with several notebooks on the servers and this is an amalgamation of the code used for the various steps.\n",
    "\n",
    "However, I noticed there were several repeated abstracts,e.g. 'Note Available' and 'N/A' occurs over 5000 times in the data set (among other repeats, or near repeats). \n",
    "\n",
    "In hindsight I should have at least removed the duplicates as it would have been a more sane approach to testing the validity of the methods.  As there are at least 6000 duplicates (and more near duplicates) most of the pairs found could have been found using the collections.Counter() method. Which defeats the purpose of this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import os \n",
    "import re \n",
    "import binascii \n",
    "from time import time\n",
    "from os import listdir\n",
    "import itertools\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://archive.ics.uci.edu/ml/machine-learning-databases/nsfabs-mld/Part1.zip\n",
    "#!wget https://archive.ics.uci.edu/ml/machine-learning-databases/nsfabs-mld/Part2.zip\n",
    "#!wget https://archive.ics.uci.edu/ml/machine-learning-databases/nsfabs-mld/Part3.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile \n",
    "for f in listdir(): \n",
    "        if '.zip' in f:\n",
    "            with zipfile.ZipFile(f, 'r') as zip_ref: \n",
    "                zip_ref.extractall() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = [] \n",
    "for root_loc in ['Part1/', 'Part 2/', 'Part 3/']:\n",
    "    for loc in listdir(root_loc):\n",
    "        for loc2 in listdir(root_loc+loc+'/'): \n",
    "            if 'html' not in loc2:\n",
    "                for file in listdir(root_loc+loc+'/'+loc2+'/'):\n",
    "                    if '.txt' in file: \n",
    "                        fnames.append(root_loc+loc+'/'+loc2+'/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 3/awards_2002/awd_2002_26/a0226848.txt 0226848 Ghazi This award will support the 3rd annual International Conference/Workshop on High Resolution Sector Field ICPMS to be held at the Department of Geology, Georgia State University, Atlanta, GA on October 2-5, 2002. This international conference is a continuation of the previous two successful meetings that were held in Norfolk, Virginia, (2000), and in Vienna, Austria (2001). There are now more than 300 single collector and nearly 100 multicollector high resolution ICP mass spectrometers around the world. We request funding to organize this meeting to bring together all the scientists around the world who work in the field of high resolution ICP mass spectrometry and accessory instrumentation (e.g., laser ablation, HPLC, CE, Microdrill techniques). This conference is intended to be a forum for the exchange of ideas and the presentation of work completed or in progress, for comment and discussion among international research groups working in the same field, as well as with research colleagues from the manufacturers to discuss future goals and improvements with the instrumentation. The conference will feature newest development in the field of high resolution plasma mass spectrometry, and we encourage participation of users from all kinds of applications. We specially encourage the participation of graduate students and post-docs. We will summarize the outcomes from this conference in the form of a report documenting directions and priorities in ICPMS and HR-ICPMS based-research. We anticipate that this report will comment on the development of useful and appropriate instrument-based topics which can be included in undergraduate and graduate geosciences curricula. *** \n"
     ]
    }
   ],
   "source": [
    "def read_file(fname): \n",
    "    with open(fname, 'r', encoding='UTF-8') as f:\n",
    "        # skip all lines until abstract \n",
    "        for line in f: \n",
    "            if \"Abstract    :\" in line:\n",
    "                break\n",
    "\n",
    "        # get abstract as a single string \n",
    "        abstract = ' '.join([line[:-1].strip() for line in f]) \n",
    "        abstract = re.sub(' +', ' ', abstract) # remove double spaces \n",
    "        return abstract\n",
    "    \n",
    "fname = fnames[130000] \n",
    "print(fname,read_file(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332 files had issues reading\n"
     ]
    }
   ],
   "source": [
    "error_messages, problem_files = [], []\n",
    " \n",
    "for file in fnames: \n",
    "    try:\n",
    "        read_file(file) \n",
    "    except Exception as e: \n",
    "            error_messages.append(e.reason)\n",
    "            problem_files.append(file)\n",
    "fnames = [file for file in fnames if file not in problem_files] \n",
    "print(f'{len(problem_files)} files had issues reading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'invalid continuation byte', 'invalid start byte'}, 332)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(error_messages), len(problem_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shingles(fname, k=5):\n",
    "    \"\"\"Get all shingles from requested file (hashes of these shingles)\n",
    "    \"\"\"\n",
    "    with open(fname, 'r') as f:\n",
    "        # skip all lines until abstract\n",
    "        for line in f:\n",
    "            if \"Abstract    :\" in line:\n",
    "                break\n",
    "\n",
    "        # get abstract as a single string\n",
    "        abstract = ' '.join([line[:-1].strip() for line in f])\n",
    "        abstract = re.sub(' +', ' ', abstract)  # remove double spaces\n",
    "\n",
    "        L = len(abstract)\n",
    "        shingles = set()  # we use a set to automatically eliminate duplicates\n",
    "        for i in range(L-k+1):\n",
    "            shingle = abstract[i:i+k]\n",
    "            crc = binascii.crc32(shingle.encode('utf-8')) #& 0xffffffff  # hash the shingle to a 32-bit integer\n",
    "            shingles.add(crc)\n",
    "        return shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: Part1/awards_1990/awd_1990_00/a9000050.txt\n",
      "number of shingles: 337\n",
      "example of shingles: [1598752772, 37695495, 502552604, 1814253598, 2860077100]\n"
     ]
    }
   ],
   "source": [
    "fname = fnames[9]\n",
    "print(\"file: {}\".format(fname))\n",
    "print(\"number of shingles: {}\".format(len(get_shingles(fname, k=5))))\n",
    "print(\"example of shingles: {}\".format(list(get_shingles(fname, k=5))[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shingles_vectors = []\n",
    "\n",
    "for file in fnames: \n",
    "    sh = list(get_shingles(file, k=5))\n",
    "    shingles_vectors.append(sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_score(x, y):\n",
    "    \"\"\"\n",
    "    Jaccard Similarity J (A,B) = | Intersection (A,B) | /\n",
    "                                    | Union (A,B) |\n",
    "    \"\"\"\n",
    "    intersection_cardinality = len(set(x).intersection(set(y)))\n",
    "    union_cardinality = len(set(x).union(set(y)))\n",
    "    return intersection_cardinality / float(union_cardinality)\n",
    "\n",
    "jaccard_similarity_score(shingles_vectors[0], shingles_vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_comparison(fnames, s=0.9, k=5):\n",
    "    t = time()\n",
    "    candidates = []\n",
    "    for pair in itertools.combinations(fnames,2):\n",
    "        js = jaccard_similarity_score(get_shingles(pair[0], k=k),get_shingles(pair[1], k=k))\n",
    "\n",
    "        if js > s:\n",
    "            candidates.append(pair)\n",
    "    t1 = time()-t\n",
    "    return candidates, t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pw_candidates5, pw_pairTime5 = pairwise_comparison(fnames, k=5)\n",
    "#pw_candidates10, pw_pairTime10 = pairwise_comparison(fnames, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"pw_dict.pckl\", \"rb\") as input_file:\n",
    "     pw_dict = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With k=5 for pairwise comparison, 15755 pairs were found, which took 14400 seconds\n",
      "With k=10 for pairwise comparison, 4442 pairs were found, which took 14400 seconds\n"
     ]
    }
   ],
   "source": [
    "for key in pw_dict:\n",
    "    print(f'With {key} for pairwise comparison, {len(pw_dict[key][0])} pairs were found, which took {round(pw_dict[key][1])} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case I had to set a time limit as it kept going over the csc time limit. What can see in this case is that k=5 found for more candidate pairs than k=10. \n",
    "\n",
    "However, having exerimented with a smaller sample of the data k=10 is a bit slower than k=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global parameters to process the whole dataset\n",
    "bands = 5\n",
    "rows = 5\n",
    "nsig = bands*rows  # number of elements in signature, or the number of different random hash functions\n",
    "\n",
    "maxShingleID = 2**32-1  # record the maximum shingle ID that we assigned\n",
    "nextPrime = 4294967311  # next prime number after maxShingleID\n",
    "\n",
    "A = numpy.random.randint(0, nextPrime, size=(nsig,))\n",
    "B = numpy.random.randint(0, nextPrime, size=(nsig,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ShingleID = list(get_shingles(fname, k=5))[0]\n",
    "\n",
    "print(\"random shingle: {}\".format(ShingleID))\n",
    "\n",
    "hashCode = ((A[0]*ShingleID + B[0]) % nextPrime) % maxShingleID\n",
    "print(\"its hash code by first hash function: {}\".format(hashCode))\n",
    "hashCode = ((A[1]*ShingleID + B[1]) % nextPrime) % maxShingleID\n",
    "print(\"its hash code by second hash function: {}\".format(hashCode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive version of Minhash algorithm that computes a signature for a single file\n",
    "# all shingles from that file are given in 'shingles'\n",
    "\n",
    "def minhash(shingles, A, B, nextPrime, maxShingleID, nsig):\n",
    "    signature = []\n",
    "    for i in range(nsig):  # number of hash functions == nsig\n",
    "        minHashCode = maxShingleID + 1\n",
    "        a = A[i]\n",
    "        b = B[i]\n",
    "        \n",
    "        for ShingleID in shingles:\n",
    "            hashCode = ((a*ShingleID + b) % nextPrime) % maxShingleID\n",
    "            if hashCode < minHashCode:\n",
    "                minHashCode = hashCode\n",
    "\n",
    "        signature.append(minHashCode)\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast implementation of Minhash algorithm\n",
    "# computes all random hash functions for a shingle at once, using vector operations\n",
    "# also finds element-wise minimum of two vectors efficiently\n",
    "def minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig):\n",
    "    signature = numpy.ones((nsig,)) * (maxShingleID + 1)\n",
    "\n",
    "    for ShingleID in shingles:\n",
    "        hashCodes = ((A*ShingleID + B) % nextPrime) % maxShingleID\n",
    "        numpy.minimum(signature, hashCodes, out=signature)\n",
    "\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(x, y):\n",
    "    from scipy.spatial.distance import cosine\n",
    "    similarity_score = 1-cosine(x, y)\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_min_hashing(fnames, s=0.9, k=5, cosine_dist=False):\n",
    "    signatures = []  # signatures for all files\n",
    "    for fname in fnames:\n",
    "        shingles = get_shingles(fname, k=k)\n",
    "        signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "        signatures.append(signature)\n",
    "\n",
    "    Nfiles = len(signatures)\n",
    "    t = time()\n",
    "    candidates = []\n",
    "    for i in range(Nfiles):\n",
    "        for j in range(i+1, Nfiles):\n",
    "            if cosine_dist:\n",
    "                Jsim = cosine_distance(signatures[i], signatures[j])\n",
    "            else:\n",
    "                Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in two vectors\n",
    "            if Jsim >= s:\n",
    "                candidates.append((i,j))\n",
    "    t2 = time() - t\n",
    "\n",
    "    return candidates, t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mh_candidates_s5, mh_pairTime_s5 = run_min_hashing(fnames, s=0.5)\n",
    "#mh_candidates_Cos, mh_pairTime_Cos = run_min_hashing(fnames, s=0.9, cosine_dist=True)\n",
    "#mh_candidates_s95, mh_pairTime_s95 = run_min_hashing(fnames, s=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mh_candidates_s5, mh_pairTime_s5 = run_min_hashing(fnames, s=0.5)\n",
    "#mh_candidates_s9, mh_pairTime_s9 = run_min_hashing(fnames, s=0.9)\n",
    "#mh_candidates_s95, mh_pairTime_s95 = run_min_hashing(fnames, s=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mh_candidates_Cos, mh_pairTime_Cos = run_min_hashing(fnames, s=0.9, cosine_dist=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"minhash_dict_3hr.pckl\", \"rb\") as input_file:\n",
    "     mh_dict2 = pickle.load(input_file)\n",
    "        \n",
    "with open(r\"minhash_dict1.pckl\", \"rb\") as input_file:\n",
    "     mh_dict1 = pickle.load(input_file)\n",
    "        \n",
    "with open(r\"cosD.pckl\", \"rb\") as input_file:\n",
    "     mh_cosD = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With k=5 for minhashing, 3798680 pairs were found, which took 14401 seconds\n",
      "With k=10 for minhashing, 3879026 pairs were found, which took 14401 seconds\n"
     ]
    }
   ],
   "source": [
    "for key in mh_dict1:\n",
    "    print(f'With {key} for minhashing, {len(mh_dict1[key][0])} pairs were found, which took {round(mh_dict1[key][1])} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had issues with timing so I used a time out of 4 hours for each. \n",
    "\n",
    "With K = 5 fewer pairs were found than for K=10. That could be because as with pairwise, k=10 is slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With s=0.5 for minhashing, 3179659 pairs were found, which took 10800 seconds\n",
      "With s=0.9 for minhashing, 3190341 pairs were found, which took 10801 seconds\n",
      "With s=0.95 for minhashing, 3180771 pairs were found, which took 10800 seconds\n"
     ]
    }
   ],
   "source": [
    "for key in mh_dict2:\n",
    "    print(f'With {key} for minhashing, {len(mh_dict2[key][0])} pairs were found, which took {round(mh_dict2[key][1])} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I used a timeout of 3 hours for each (I ran the analysis for these 3 with a limit of 10 hours so this would give me time to save the results).\n",
    "\n",
    "The number of pairs are misleading as the higher the _S_ threshold the fewer pairs should be found. This is because it is a stricter threshold. However, due to the timeout limit, this is not reflected in the numbers of pairs.\n",
    "\n",
    "K=5 was used for all variations of S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With k=5 for minhashing and Cosine distance (similarity), 2861753 pairs were found, which took 28800 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f'With k=5 for minhashing and Cosine distance (similarity), {len(mh_cosD[0])} pairs were found, which took {round(mh_cosD[1])} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the minhashing with cosine distance I calculated cosine distance and subtracted it from 1 (i.e. 1-cosine_dist). 286k pairs were found and it took 8 hours (my timeout limit for this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSH(signatures, bands, rows, Ab, Bb, nextPrime, maxShingleID):\n",
    "    \"\"\"Locality Sensitive Hashing\n",
    "    \"\"\"\n",
    "    numItems = signatures.shape[1]\n",
    "    signBands = numpy.array_split(signatures, bands, axis=0)\n",
    "    candidates = set()\n",
    "    for nb in range(bands):\n",
    "        hashTable = {}\n",
    "        for ni in range(numItems):\n",
    "            item = signBands[nb][:,ni]\n",
    "            hash = (numpy.dot(Ab[nb,:], item) + Bb[nb]) % nextPrime % maxShingleID\n",
    "            if hash not in hashTable:\n",
    "                hashTable[hash] = [ni]\n",
    "            else:\n",
    "                hashTable[hash].append(ni)\n",
    "        for _,items in hashTable.items():\n",
    "            if len(items) > 1:\n",
    "                L = len(items)\n",
    "                for i in range(L-1):\n",
    "                    for j in range(i+1, L):\n",
    "                        cand = [items[i], items[j]]\n",
    "                        numpy.sort(cand)\n",
    "                        candidates.add(tuple(cand))\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LSH(fnames, s=.9, k=5):\n",
    "\n",
    "    # find candidates with LSH\n",
    "\n",
    "    signatures = []  # signatures for all files\n",
    "    for fname in fnames:\n",
    "        shingles = get_shingles(fname, k=5)\n",
    "        signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "        signatures.append(signature)\n",
    "\n",
    "    # prepare data for LSH\n",
    "    A2 = numpy.random.randint(0, nextPrime/2, size=(bands, rows))  # now we need a vector of A parameters for each band\n",
    "    B2 = numpy.random.randint(0, nextPrime/2, size=(bands, ))\n",
    "    signatures = numpy.array(signatures).T  # LSH needs a matrix of signatures, not a list of vectors\n",
    "\n",
    "    s = 0.95  # similarity threshold\n",
    "    Nfiles = signatures.shape[1]  # number of different files\n",
    "    t = time()\n",
    "    candidates = LSH(signatures, bands, rows, A2, B2, nextPrime, maxShingleID)\n",
    "    t2 = time() - t\n",
    "    \n",
    "    return candidates, t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lsh_candidates25, lsh_pairTime25 = get_LSH(fnames, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global parameters to process the whole dataset\n",
    "bands = 10\n",
    "rows = 10\n",
    "nsig = bands*rows  # number of elements in signature, or the number of different random hash functions\n",
    "\n",
    "maxShingleID = 2**32-1  # record the maximum shingle ID that we assigned\n",
    "nextPrime = 4294967311  # next prime number after maxShingleID\n",
    "\n",
    "A = numpy.random.randint(0, nextPrime, size=(nsig,))\n",
    "B = numpy.random.randint(0, nextPrime, size=(nsig,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lsh_candidates100, lsh_pairTime100 = get_LSH(fnames, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"lsh_dict_K.pckl\", \"rb\") as input_file:\n",
    "     lsh_dict_K = pickle.load(input_file)\n",
    "        \n",
    "with open(r\"lsh_dict_100_2.pckl\", \"rb\") as input_file:\n",
    "     lsh_dict_100 = pickle.load(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With k=5 and 25 rows & bands for LSH, 159049 pairs were found, which took 1256 seconds\n",
      "With k=10 and 25 rows & bands for LSH, 159048 pairs were found, which took 855 seconds\n"
     ]
    }
   ],
   "source": [
    "for key in lsh_dict_K:\n",
    "    print(f'With {key} and 25 rows & bands for LSH, {len(lsh_dict_K[key][0])} pairs were found, which took {round(lsh_dict_K[key][1])} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 5 rows and 5 bands, k=5 took 1256 seconds and found 159049 pairs while k=10 was faster with 855 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With k=5 and 100 rows & bands for LSH, 13837748 pairs were found, which took 2476 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f'With k=5 and 100 rows & bands for LSH, {len(lsh_dict_100[0])} pairs were found, which took {round(lsh_dict_100[1])} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using k=5 and 10 rows and 10 bands, 13837748 pairs were found and it took 2476 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_1NN(candidates):\n",
    "    cluster_dict = {}\n",
    "    for pairs in list(candidates):\n",
    "        p1, p2 = pairs\n",
    "        if p1 not in cluster_dict:\n",
    "            cluster_dict[p1]=[p1, p2]\n",
    "        else:\n",
    "            cluster_dict[p1]+=[p2]\n",
    "    return cluster_dict    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 1-NN I will use k=5 for pairwise, minhashing and LSH. For LSH I will use 5x5 rows and bands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_pw = find_1NN(pw_dict['k=5'][0])\n",
    "clusters_mh = find_1NN(mh_dict2['s=0.95'][0])\n",
    "clusters_lsh = find_1NN(lsh_dict_K['k=5'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part1/awards_1990/awd_1990_00/a9000054.txt 2\n",
      "Part1/awards_1990/awd_1990_00/a9000132.txt 2\n",
      "Part1/awards_1990/awd_1990_00/a9000177.txt 3\n",
      "Part1/awards_1990/awd_1990_00/a9000201.txt 2\n",
      "Part1/awards_1990/awd_1990_00/a9000221.txt 5252\n",
      "Part1/awards_1990/awd_1990_00/a9000222.txt 5251\n",
      "Part1/awards_1990/awd_1990_00/a9000223.txt 5250\n"
     ]
    }
   ],
   "source": [
    "for key in clusters_pw:\n",
    "    print(key, len(clusters_pw[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part1/awards_1990/awd_1990_00/a9000221.txt Not Available\n",
      "\n",
      "\n",
      "\n",
      "Part1/awards_1990/awd_1990_00/a9000222.txt Not Available\n",
      "\n",
      "\n",
      "\n",
      "Part1/awards_1990/awd_1990_00/a9000223.txt Not Available\n",
      "\n",
      "\n",
      "\n",
      "Part1/awards_1990/awd_1990_00/a9000396.txt Not Available\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "key = 'Part1/awards_1990/awd_1990_00/a9000221.txt' \n",
    "ls = clusters_pw[key]\n",
    "for n in ls[:4]:\n",
    "    print(n,read_file(n))\n",
    "    print('\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned earlier there are a lot of texts that simply state 'Not available' (there are at least 5250ish). It seems that due to my timeout limit of 4 hours, only 7 texts were compared to all items. However, lets look at the ones that found fewer pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part1/awards_1990/awd_1990_00/a9000201.txt  The three classes of symbiont-bearing sarcodines (acantharia, polycystine radiolaria and foraminifera) are a conspicuous component of upper ocean communities. Most of the previous research on the ecology of these taxa has been concentrated on a single species or limited group of species. Dr. Caron and colleagues will simultaneously examine the ecological role of the entire assemblage of these three protozoa. Their research examines two hypotheses: 1. In nutrient-poor oceanic environments, the symbiont primary production in the entire population of acantharia, polycystine radiolaria and foraminifera will represent a significant fraction of the total primary production and will dominate the production by large cells (operationally defined as cells larger than 74um). 2. Sarcodines will be a disproportionately important component of sinking material from the euphotic zone compared with their suspended biomass. Symbiont production rates of the entire population of symbiont-bearing sarcodines are determined through a combination of large volume and diver-collected production measurements to evaluate the importance of this production relative to total primary production. The significance of these planktonic protists for the vertical flux of material is estimated by the concurrent measurement of both the stocks of sarcodines and the rate of sinking export. These measurements are coordinated with the GOFS Time-Series Study at Bermuda (participation in two cruises per year), which enables the investigators to view their results from a large, seasonal perspective.\n",
      "\n",
      "\n",
      "\n",
      "Part1/awards_1990/awd_1990_17/a9017173.txt  The three classes of symbiont-bearing sarcodines (acantharia, polycystine radiolaria and foraminifera) are a conspicuous component of upper ocean communities. Most of the previous research on the ecology of these taxa has been concentrated on a single species or limited group of species. Dr. Michaels and colleagues will simultaneously examine the ecological role of the entire assemblage of these three protozoa. Their research examines two hypotheses: 1. In nutrient-poor oceanic environments, the symbiont primary production in the entire population of acantharia, polycystine radiolaria and foraminifera will represent a significant fraction of the total primary production and will dominate the production by large cells (operationally defined as cells larger than 74um). 2. Sarcodines will be a disproportionately important component of sinking material from the euphotic zone compared with their suspended biomass. Symbiont production rates of the entire population of symbiont-bearing sarcodines are determined through a combination of large volume and diver-collected production measurements to evaluate the importance of this production relative to total primary production. The significance of these planktonic protests for the vertical flux of material is estimated by the concurrent measurement of both the stocks of sarcodines and the rate of sinking export. These measurements are coordinated with the GOFS Time-Series Study at Bermuda (participation in two cruises per year), which enables the investigators to view their results from a large, seasonal perspective.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "key = 'Part1/awards_1990/awd_1990_00/a9000201.txt' \n",
    "ls = clusters_pw[key]\n",
    "for n in ls:\n",
    "    print(n,read_file(n))\n",
    "    print('\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we appear to have found duplicates. Perhaps I should have removed the duplicates from the data set in order to properly test this method, though I suspect a would still have had timout issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_keys' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-610753fbee45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclusters_mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'dict_keys' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "clusters_mh.keys()[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 2\n",
      "32 2\n",
      "52 3\n",
      "61 2\n",
      "67 5252\n",
      "68 5251\n",
      "69 5250\n",
      "85 2\n",
      "106 45\n",
      "111 2\n"
     ]
    }
   ],
   "source": [
    "for key in list(clusters_mh.keys())[:10]:\n",
    "    print(key, len(clusters_mh[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in clusters_mh:\n",
    "    ls = clusters_mh[key]\n",
    "\n",
    "    text1 = read_file(fnames[ls[0]]).strip()\n",
    "    text2 = read_file(fnames[ls[1]]).strip()\n",
    "    if text1==text2 or key==32:\n",
    "        continue\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The three classes of symbiont-bearing sarcodines (acantharia, polycystine radiolaria and foraminifera) are a conspicuous component of upper ocean communities. Most of the previous research on the ecology of these taxa has been concentrated on a single species or limited group of species. Dr. Caron and colleagues will simultaneously examine the ecological role of the entire assemblage of these three protozoa. Their research examines two hypotheses: 1. In nutrient-poor oceanic environments, the symbiont primary production in the entire population of acantharia, polycystine radiolaria and foraminifera will represent a significant fraction of the total primary production and will dominate the production by large cells (operationally defined as cells larger than 74um). 2. Sarcodines will be a disproportionately important component of sinking material from the euphotic zone compared with their suspended biomass. Symbiont production rates of the entire population of symbiont-bearing sarcodines are determined through a combination of large volume and diver-collected production measurements to evaluate the importance of this production relative to total primary production. The significance of these planktonic protists for the vertical flux of material is estimated by the concurrent measurement of both the stocks of sarcodines and the rate of sinking export. These measurements are coordinated with the GOFS Time-Series Study at Bermuda (participation in two cruises per year), which enables the investigators to view their results from a large, seasonal perspective.',\n",
       " 'Length: 1579',\n",
       " 'The three classes of symbiont-bearing sarcodines (acantharia, polycystine radiolaria and foraminifera) are a conspicuous component of upper ocean communities. Most of the previous research on the ecology of these taxa has been concentrated on a single species or limited group of species. Dr. Michaels and colleagues will simultaneously examine the ecological role of the entire assemblage of these three protozoa. Their research examines two hypotheses: 1. In nutrient-poor oceanic environments, the symbiont primary production in the entire population of acantharia, polycystine radiolaria and foraminifera will represent a significant fraction of the total primary production and will dominate the production by large cells (operationally defined as cells larger than 74um). 2. Sarcodines will be a disproportionately important component of sinking material from the euphotic zone compared with their suspended biomass. Symbiont production rates of the entire population of symbiont-bearing sarcodines are determined through a combination of large volume and diver-collected production measurements to evaluate the importance of this production relative to total primary production. The significance of these planktonic protests for the vertical flux of material is estimated by the concurrent measurement of both the stocks of sarcodines and the rate of sinking export. These measurements are coordinated with the GOFS Time-Series Study at Bermuda (participation in two cruises per year), which enables the investigators to view their results from a large, seasonal perspective.',\n",
       " 'Length: 1582')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1, f'Length: {len(text1)}', text2 , f'Length: {len(text2)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By filtering out duplicates, it seems that our method also found near duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27062 40\n",
      "67882 4\n",
      "22807 14\n",
      "16563 7\n",
      "7487 163\n",
      "19055 32\n",
      "32154 2\n",
      "20057 30\n",
      "41472 35\n",
      "51925 7\n"
     ]
    }
   ],
   "source": [
    "for key in list(clusters_lsh.keys())[:10]:\n",
    "    print(key, len(clusters_lsh[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in clusters_lsh:\n",
    "    ls = clusters_lsh[key]\n",
    "\n",
    "    text1 = read_file(fnames[ls[0]]).strip()\n",
    "    text2 = read_file(fnames[ls[1]]).strip()\n",
    "    if text1==text2 or key==32:\n",
    "        continue\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('9312570 Vander This project will develop demonstrational, visual programming techniques and determine in which contexts programmers prefer them to textual programming techniques. An interactive, two-view environment will be provided in which users can create programs by manipulating concrete pictorial, examples of data structures or by entering code into a textual editor. Users will be able to create journal- quality pictures of data structures, such as arrays, lists, trees, and graphs, fill in example values. The system will use various inferencing strategies to construct appropriate general-purpose code. Users will be able to move back and forth between the visual and textual editors, using whichever representation is most natural. By providing both a visual and a textual representation, it should be possible to determine those operations which a programmer views as intrinsically textual. Ultimately these observations should allow better programming environments to be built that provide alternative ways of creating programs. ***',\n",
       " 'Detritus is present in all aquatic ecosystems and yet there is still much debate about its importance in supporting the growth of large organisms, such as invertebrates and fish. For example, mangrove detritus may not be utilized directly by shrimp but it may support a rich microbial assemblage that ultimately supports shrimp growth. This study will test the hypothesis that the foodweb based on mangrove detritus is important in supporting the growth of post-larval penaeaid shrimp. The proposed research will be carried out in a Costa Rican estuary which has large shrimp fisheries and a biological station. Although early studies using stomach analysis emphasized the importance of mangrove detritus in supporting shrimp growth, more recent studies have challenged this. To help resolve this controversy, stomach contents and stable isotope (C, N and S) analysis and mesocosm experiments will be conducted to examine shrimp growth and possible food chain isotope fractionation. By combining tools in invertebrate biology, microbiology and geochemistry it may be possible to determine the ultimate source of primary production (mangroves, phytoplankton or benthic algae) supporting growth of postlarval shrimp. This research will also examine whether the microbial loop, here supported by mangrove detritus, is a source or sink for the biomass production of higher trophic levels.',\n",
       " \"This new course combines concepts from the students' prior course work and introduces them to a broad range of modern design and testing techniques for advanced materials. Fundamental engineering concepts are followed immediately with engineering problem-solving. A series of lab sessions have been formulated, each involving a task directed toward the reverse-engineering, evaluation, and improvement of a sample, manufactured mechanical product. Design and manufacture is approached as parallel rather than sequential tasks. The course also enhances learning by increasing the level of interaction on various levels. The course is arranged in a way which emphasizes discussion and oral examination rather than lecture. The instructor spends most of his/her time interacting with small groups of students rather than the entire class. The students are required to interact amongst each other in the organization and preparation of the lab reports. Unlike more traditional courses, grading will not be based strictly upon knowledge level. Student interest in the material will be stimulated because the proposed lab sessions have both a sequence and a unifying context which give the learning experience continuity. The course will give the students exposure to more modern and advanced mechanical testing techniques, engineering materials, and approaches to the design/manufacturing interaction than they currently receive.\")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_file(fnames[ls[2]]).strip(), text1, text2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSH worked a seems to have worked a bit better, it found duplicates and near duplicates. However, we can see here that it also found a pairs of scientific papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the number of pairs found and the number of duplicates I could simply do the following and gain a similar number pairs. This was my own stupidity. I have learnt my lesson and will make sure I inspect the data more efficiently before I start any analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Not Available', 5252),\n",
       " ('The Mathematical Sciences Postdoctoral Research Fellowships are awards to recent recipients of doctoral degrees in the mathematical sciences. These awards are a means of contributing to the future vit',\n",
       "  137),\n",
       " ('This is a contract', 97),\n",
       " ('', 69),\n",
       " (\"9394512 Williams This is an abstract. It doesn't matter what the margin settings are, nor will you have to worry about length or the three asterisks at the bottom. The new abstract load will take any\",\n",
       "  62),\n",
       " ('Postdoctoral Fellowship', 38),\n",
       " ('This is a contract.', 34),\n",
       " ('This award will support study of the Japanese language by an American scientist or engineer by providing a stipend, tuition, or other course-related expenses. The objectives of the program are to enab',\n",
       "  28),\n",
       " ('Mathematical Sciences Fellowship', 24),\n",
       " ('This award is made under the innovative technology connections portion of NCRI\\'s \"Connections to the Internet\" announcement, NSF 96-64, which covers K-12 education institutions (including vocational t',\n",
       "  23)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "abstract = []\n",
    "for file in fnames:\n",
    "    abstract.append(read_file(file)[:200].strip())\n",
    "    \n",
    "c = Counter(abstract)\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should have removed these duplicates before starting to more sanely compare the methods. By just looking at the documents with 'Not Available' would have found 13789126 pairs. Considering the timeouts I employed most of the comparisons found were these type of repeats. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
