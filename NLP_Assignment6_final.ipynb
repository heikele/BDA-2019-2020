{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import os \n",
    "import re \n",
    "import binascii \n",
    "from time import time\n",
    "from os import listdir\n",
    "from os import walk\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install tensorflow_hub tensorflow\n",
    "#!pip3 install cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-16 16:07:03--  https://archive.ics.uci.edu/ml/machine-learning-databases/nsfabs-mld/Part1.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 68040342 (65M) [application/x-httpd-php]\n",
      "Saving to: ‘Part1.zip’\n",
      "\n",
      "Part1.zip           100%[===================>]  64.89M  15.6MB/s    in 5.4s    \n",
      "\n",
      "2020-03-16 16:07:09 (11.9 MB/s) - ‘Part1.zip’ saved [68040342/68040342]\n",
      "\n",
      "--2020-03-16 16:07:10--  https://archive.ics.uci.edu/ml/machine-learning-databases/nsfabs-mld/Part2.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 72816384 (69M) [application/x-httpd-php]\n",
      "Saving to: ‘Part2.zip’\n",
      "\n",
      "Part2.zip           100%[===================>]  69.44M  15.7MB/s    in 5.7s    \n",
      "\n",
      "2020-03-16 16:07:16 (12.1 MB/s) - ‘Part2.zip’ saved [72816384/72816384]\n",
      "\n",
      "--2020-03-16 16:07:17--  https://archive.ics.uci.edu/ml/machine-learning-databases/nsfabs-mld/Part3.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50699151 (48M) [application/x-httpd-php]\n",
      "Saving to: ‘Part3.zip’\n",
      "\n",
      "Part3.zip           100%[===================>]  48.35M  12.7MB/s    in 4.4s    \n",
      "\n",
      "2020-03-16 16:07:23 (11.1 MB/s) - ‘Part3.zip’ saved [50699151/50699151]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/nsfabs-mld/Part1.zip\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/nsfabs-mld/Part2.zip\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/nsfabs-mld/Part3.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile \n",
    "for f in listdir(): \n",
    "        if '.zip' in f:\n",
    "            with zipfile.ZipFile(f, 'r') as zip_ref: \n",
    "                zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_loc = \"Part 3/awards_2002/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = [] \n",
    "for root_loc in ['Part 3/',]:\n",
    "    for loc in listdir(root_loc):\n",
    "        if loc == 'awards_2002':\n",
    "            for loc2 in listdir(root_loc+loc+'/'): \n",
    "                if 'html' not in loc2:\n",
    "                    for file in listdir(root_loc+loc+'/'+loc2+'/'):\n",
    "                        if '.txt' in file: \n",
    "                            fnames.append(root_loc+loc+'/'+loc2+'/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 3/awards_2002/awd_2002_03/a0203832.txt This project addresses local vibrational mode (LVM) spectroscopy of semiconductors such as GaN, ZnO, Si, and CdTe in order to determine the structure of defect centers and their vibrational interactions with the host environment. Impurities such as hydrogen, oxygen, and DX centers will be studied in these technologically important materials. Hydrostatic pressure will be used, with infrared (IR) and Raman spectroscopy, to probe the LVMs arising from such impurities. In addition to inorganic semiconductors, organic semiconductors such as pentacene will also be investigated. Time-resolved experiments using ultrafast IR spectroscopy will be used to probe vibrational lifetimes and resonant interactions in semiconductors. The project addresses fundamental research issues in a topical area of electronic/photonic materials science having high technological relevance. An important feature of the project is the strong emphasis on education, and the integration of research and education. The proposed educational activities will benefit a range of students from the eighth grade to graduate school. Educational programs integrated with research include outreach activities with a public school on the Colville Indian reservation, improvements to undergraduate physics courses, and the development of laboratories and lecture demonstrations. These activities are pursued to encourage students from diverse backgrounds to enter fields related to materials science and solid-state physics. \n"
     ]
    }
   ],
   "source": [
    "def read_file(fname): \n",
    "    with open(fname, 'r', encoding='UTF-8') as f:\n",
    "        # skip all lines until abstract \n",
    "        for line in f: \n",
    "            if \"Abstract    :\" in line:\n",
    "                break\n",
    "\n",
    "        # get abstract as a single string \n",
    "        abstract = ' '.join([line[:-1].strip() for line in f]) \n",
    "        abstract = re.sub(' +', ' ', abstract) # remove double spaces \n",
    "        return abstract\n",
    "    \n",
    "fname = fnames[1000] \n",
    "print(fname,read_file(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 files had issues reading\n"
     ]
    }
   ],
   "source": [
    "error_messages, problem_files = [], []\n",
    " \n",
    "for file in fnames: \n",
    "    try:\n",
    "        read_file(file) \n",
    "    except Exception as e: \n",
    "            error_messages.append(e.reason)\n",
    "            problem_files.append(file)\n",
    "fnames = [file for file in fnames if file not in problem_files] \n",
    "print(f'{len(problem_files)} files had issues reading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'invalid continuation byte', 'invalid start byte'}, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(error_messages), len(problem_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = [read_file(f) for f in fnames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Topic Anlysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9918, 20000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Calculate TF-IDF scores on a corpus\n",
    "\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Set parameters and initialize\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, use_idf=True, sublinear_tf=True, max_df=1.0,\n",
    "                                   ngram_range=(1, 3), max_features=20000)\n",
    "# Tip: the vectorizer also supports extracting n-gram features (common short sequences of words), which may be more descriptive but also much less frequent\n",
    "\n",
    "# Calcualate term-document matrix with tf-idf scores\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(abstracts)\n",
    "\n",
    "# Check matrix shape\n",
    "tfidf_matrix.toarray().shape # N_docs x N_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '000 years', '000 years ago', '01', '01 157', '01 157 category', '02', '10', '10 000', '100']\n",
      "['yr', 'zealand', 'zero', 'zircon', 'zonal', 'zone', 'zone in', 'zone of', 'zones', 'zooplankton']\n"
     ]
    }
   ],
   "source": [
    "# Inspect terms in vocabulary\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "print(tfidf_vectorizer.get_feature_names()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9632\tthe\n",
      "9614\tof\n",
      "9608\tand\n",
      "9506\tto\n",
      "9437\tin\n",
      "8739\tthis\n",
      "8621\tfor\n",
      "8264\tis\n",
      "8223\twill\n",
      "7943\tof the\n",
      "7627\tbe\n",
      "7415\ton\n",
      "7267\twith\n",
      "7163\tthat\n",
      "6653\tare\n",
      "6639\tresearch\n",
      "6633\tin the\n",
      "6558\tby\n",
      "6421\tas\n",
      "5964\tfrom\n"
     ]
    }
   ],
   "source": [
    "## Inspect document frequencies (counts) of terms\n",
    "\n",
    "from collections import Counter\n",
    "terms_in_docs = tfidf_vectorizer.inverse_transform(tfidf_matrix)\n",
    "token_counter = Counter()\n",
    "for terms in terms_in_docs:\n",
    "    token_counter.update(terms)\n",
    "\n",
    "for term, count in token_counter.most_common(20):\n",
    "    print(\"%d\\t%s\" % (count, term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 0, top terms by TF-IDF\n",
      "0.15\talgebraic\n",
      "0.14\talgebraic geometry\n",
      "0.14\tgroup is\n",
      "0.12\ttask of\n",
      "0.12\tthe task\n",
      "\n",
      "Document 1, top terms by TF-IDF\n",
      "0.14\tcontrol\n",
      "0.13\tstudents to\n",
      "0.13\tthis project in\n",
      "0.13\tthat should\n",
      "0.13\tamerican\n",
      "\n",
      "Document 2, top terms by TF-IDF\n",
      "0.22\tupdating\n",
      "0.14\treference\n",
      "0.13\tsecondly\n",
      "0.13\tmodel based\n",
      "0.13\tmethod\n",
      "\n",
      "Document 3, top terms by TF-IDF\n",
      "0.17\tgroup theory\n",
      "0.14\topen problems\n",
      "0.14\tconference\n",
      "0.13\tto learn\n",
      "0.13\tthe conference\n",
      "\n",
      "Document 4, top terms by TF-IDF\n",
      "0.16\tengineering design\n",
      "0.14\tuncertainty in\n",
      "0.13\tuncertainties\n",
      "0.13\tpreferences\n",
      "0.12\talternative\n"
     ]
    }
   ],
   "source": [
    "## Inspect top terms per document\n",
    "\n",
    "features = tfidf_vectorizer.get_feature_names()\n",
    "for doc_i in range(5):\n",
    "    print(\"\\nDocument %d, top terms by TF-IDF\" % doc_i)\n",
    "    for term, score in sorted(list(zip(features,tfidf_matrix.toarray()[doc_i])), key=lambda x:-x[1])[:5]:\n",
    "        print(\"%.2f\\t%s\" % (score, term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments:\n",
    "I chose to use an ngram range of 1-3 because the groupings appear more accurate. For example plenty of abstracts could mention 'group' in isolation however, some abstracts refer to 'group theory'. Which would refer to a topic as opposed to simply mentioning the word group in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_sample = tfidf_matrix[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=30, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=123, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Do clustering\n",
    "km = KMeans(n_clusters=30, random_state=123, verbose=0)\n",
    "km.fit(matrix_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq, numpy as np\n",
    "\n",
    "# Custom function to print top keywords for each cluster\n",
    "\n",
    "\n",
    "def print_clusters(matrix, clusters, n_keywords=10, rmin=0, rmax=10):\n",
    "    labels = [\"Shipping\", \"Physics and Frequencies\", \"Mathematics and 'Z's\", \"Engineering\", \n",
    "                  \"Geometry\", \"Faculty Related\", \"Visiting Research Grant\", \"Mathematics\", \n",
    "                  \"Biological Informatics Conference\", \"Microbial Biology\"]\n",
    "    \n",
    "    for cluster in range(rmin, rmax):\n",
    "        label = labels[cluster]\n",
    "        \n",
    "        cluster_docs = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        print(f\"Cluster ({label}): %d (%d docs)\" % (cluster, len(cluster_docs)))\n",
    "        \n",
    "        # Keep scores for top n terms\n",
    "        new_matrix = np.zeros((len(cluster_docs), matrix.shape[1]))\n",
    "        for cluster_i, doc_vec in enumerate(matrix[cluster_docs].toarray()):\n",
    "            for idx, score in heapq.nlargest(n_keywords, enumerate(doc_vec), key=lambda x:x[1]):\n",
    "                new_matrix[cluster_i][idx] = score\n",
    "\n",
    "        # Aggregate scores for kept top terms\n",
    "        keywords = heapq.nlargest(n_keywords, zip(new_matrix.sum(axis=0), features))\n",
    "        print(', '.join([w for s,w in keywords]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster (Shipping): 0 (13 docs)\n",
      "oceanographic, ship, vessel, shared use, operated by, profiler, shipboard, equipment, instrumentation, marine\n",
      "\n",
      "Cluster (Physics and Frequencies): 1 (29 docs)\n",
      "operators, spaces, operator, harmonic, required, no, harmonic analysis, abstract, non commutative, wavelets\n",
      "\n",
      "Cluster (Mathematics and 'Z's): 2 (26 docs)\n",
      "mathematical sciences, fellowship, mathematical, sciences, fellowships, zooplankton, zones, zone of, zone in, zone\n",
      "\n",
      "Cluster (Engineering): 3 (27 docs)\n",
      "cad, platform, superconductors, platforms, micro scale, are becoming, parameterization, deposition, chips, desktop\n",
      "\n",
      "Cluster (Geometry): 4 (37 docs)\n",
      "detector, geometry, lemurs, kaehler, manifolds, riemannian, experiment, particle, cp, solar\n",
      "\n",
      "Cluster (Faculty Related): 5 (50 docs)\n",
      "community college, teachers, teacher, alliance, teacher preparation, college faculty, science and mathematics, college, mathematics and science, and mathematics\n",
      "\n",
      "Cluster (Visiting Research Grant): 6 (34 docs)\n",
      "abroad, research fellowship, twenty four, fellowship, dr, twenty, with dr, co2, exotic, archaeological\n",
      "\n",
      "Cluster (Mathematics): 7 (26 docs)\n",
      "algebras, representation theory, conformal, representation, lie, quantum, algebras and, theory, symmetries, field theory\n",
      "\n",
      "Cluster (Biological Informatics Conference): 8 (55 docs)\n",
      "workshop, conference, meeting, the conference, the workshop, biological informatics, meetings, in biological informatics, japan, informatics\n",
      "\n",
      "Cluster (Microbial Biology): 9 (20 docs)\n",
      "microbial biology, in microbial biology, in microbial, microbial, fellowship, biology, bacterial, fungal, training, fungi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_clusters(matrix_sample, km.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments:\n",
    "\"Shipping\" and \"Physics and Frequencies\" clusters I consider good clusters they seem to have well defined topics  while the \"Mathematics and 'Z's\" and \"Visiting Research Grant\" clusters are ill defined by scanning the word lists (e.g. zooplankgton, zones and mathematics))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I had ginsem working but at some point I could no longer install it or get it working. So I am going to have to submit it with very little playing around of the settings as I was not able to resolve this issue later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google.cloud\n",
      "  Using cached https://files.pythonhosted.org/packages/ba/b1/7c54d1950e7808df06642274e677dbcedba57f75307adf2e5ad8d39e5e0e/google_cloud-0.34.0-py2.py3-none-any.whl\n",
      "Installing collected packages: google.cloud\n",
      "Successfully installed google.cloud\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ginsem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Topic modeling demo\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Fast and simple tokenization\n",
    "new_vectorizer = TfidfVectorizer(min_df=2, use_idf=True, sublinear_tf=True, max_df=1.0,stop_words='english',\n",
    "                                   ngram_range=(1, 1), max_features=20000)\n",
    "\n",
    "word_tokenizer = new_vectorizer.build_tokenizer()\n",
    "tokenized_text = [word_tokenizer(doc) for doc in abstracts]\n",
    "\n",
    "# Train LDA model\n",
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "lda_model = models.LdaModel(lda_corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "will (0.0077),  theory (0.0047),  research (0.0045),  project (0.0045),  which (0.0042),  models (0.0037),  study (0.0035),  problems (0.0034),  these (0.0032),  have (0.0030),  \n",
      "\n",
      "Topic 1\n",
      "will (0.0020),  anthracis (0.0010),  research (0.0008),  In (0.0008),  project (0.0008),  which (0.0007),  outbreak (0.0006),  models (0.0005),  these (0.0004),  their (0.0004),  \n",
      "\n",
      "Topic 2\n",
      "Not (0.0402),  Available (0.0393),  will (0.0004),  project (0.0001),  research (0.0001),  which (0.0001),  used (0.0001),  these (0.0001),  systems (0.0001),  such (0.0001),  \n",
      "\n",
      "Topic 3\n",
      "will (8.7537),  these (4.3134),  codes (4.2415),  used (4.0622),  problems (3.7306),  both (3.7025),  using (3.6732),  design (3.6266),  project (3.6013),  research (3.5857),  \n",
      "\n",
      "Topic 4\n",
      "lake (0.0026),  research (0.0010),  will (0.0010),  have (0.0004),  these (0.0004),  theory (0.0004),  which (0.0003),  has (0.0003),  problems (0.0003),  project (0.0003),  \n",
      "\n",
      "Topic 5\n",
      "will (0.0156),  research (0.0063),  project (0.0045),  materials (0.0038),  high (0.0037),  based (0.0033),  systems (0.0033),  design (0.0031),  such (0.0031),  new (0.0030),  \n",
      "\n",
      "Topic 6\n",
      "will (0.0147),  species (0.0078),  these (0.0052),  research (0.0045),  have (0.0040),  plant (0.0035),  project (0.0035),  genes (0.0032),  proteins (0.0031),  protein (0.0031),  \n",
      "\n",
      "Topic 7\n",
      "will (0.0167),  research (0.0074),  project (0.0043),  students (0.0040),  data (0.0029),  have (0.0026),  these (0.0025),  their (0.0025),  new (0.0025),  University (0.0025),  \n",
      "\n",
      "Topic 8\n",
      "research (0.0026),  problems (0.0022),  project (0.0016),  has (0.0016),  will (0.0016),  methods (0.0014),  which (0.0013),  these (0.0011),  data (0.0010),  new (0.0010),  \n",
      "\n",
      "Topic 9\n",
      "will (0.0013),  plants (0.0007),  3x (0.0007),  8x (0.0006),  research (0.0005),  6x (0.0005),  7x (0.0005),  project (0.0004),  two (0.0004),  seed (0.0003),  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect topics\n",
    "for i, topic in lda_model.show_topics(num_words=50, formatted=False):\n",
    "    print(\"Topic\", i)\n",
    "    printed_terms = 0\n",
    "    form = '{} ({}),  '\n",
    "    results = ''\n",
    "    for term, score in topic:\n",
    "        if printed_terms >= 10:\n",
    "            break\n",
    "        elif term in \"the of and to for in or The is be may an a that with at are on by this This as from can\".split():\n",
    "            continue\n",
    "        results = results+form.format(*[term, str(score)[:6]])\n",
    "        printed_terms += 1\n",
    "    print(results)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n",
    "It seems to not have worked as well as the previous topic clustering based on the highlighted words. Of course it is possible that it is more accurate if I were to read the actual grouped topics, but as I mentioned ginsem stopped working for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word Vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, use_idf=True, sublinear_tf=True, max_df=1.0,stop_words='english',\n",
    "                                   ngram_range=(1, 1), max_features=20000)\n",
    "\n",
    "# Calcualate term-document matrix with tf-idf scores\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(abstracts)\n",
    "\n",
    "# Check matrix shape\n",
    "tfidf_matrix.toarray().shape # N_docs x N_terms\n",
    "\n",
    "features = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ind = [random.randint(1000, len(features)-5000) for x in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3495, 13512, 8054, 6624, 7422]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coercion', 'perception', 'grand', 'examinations', 'formulations']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[features[ind] for ind in word_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar terms to: coercion\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1.0, 'coercion'),\n",
       " (0.4162951401544288, 'nullification'),\n",
       " (0.41629514015442876, 'unfair'),\n",
       " (0.41629514015442876, 'acquittals'),\n",
       " (0.4126745721735952, 'monogamy'),\n",
       " (0.38826472186160677, 'monogamous'),\n",
       " (0.3878467498225653, 'backed'),\n",
       " (0.37701545327718994, 'reconciliation'),\n",
       " (0.35547187094003135, 'verdicts'),\n",
       " (0.3486477223335004, 'nullify')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "\n",
    "ind = word_ind[0]\n",
    "print(\"Similar terms to:\", features[ind])\n",
    "# Get most similar terms according to the cosine similarity of their vectors (columns in the term-document matrix)\n",
    "heapq.nlargest(10, zip(cosine_similarity(tfidf_matrix[:,ind].todense().T, tfidf_matrix.todense().T)[0], features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar terms to: perception\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.9999999999999999, 'perception'),\n",
       " (0.2365060769520172, 'perceptual'),\n",
       " (0.2275446533725416, 'facial'),\n",
       " (0.22355833667115121, 'auditory'),\n",
       " (0.21372262800705172, 'luminance'),\n",
       " (0.2011062603533762, 'deception'),\n",
       " (0.17805056112668524, 'speech'),\n",
       " (0.17259028153880743, 'shelled'),\n",
       " (0.17114620589901827, 'psychophysical'),\n",
       " (0.1664457461718682, 'perceived')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = word_ind[1]\n",
    "print(\"Similar terms to:\", features[ind])# Get most similar terms according to the cosine similarity of their vectors (columns in the term-document matrix)\n",
    "heapq.nlargest(10, zip(cosine_similarity(tfidf_matrix[:,ind].todense().T, tfidf_matrix.todense().T)[0], features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar terms to: grand\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1.0000000000000002, 'grand'),\n",
       " (0.3858555058648221, 'canyon'),\n",
       " (0.35039181164825167, 'shallowness'),\n",
       " (0.2819828099901163, 'successions'),\n",
       " (0.2563079068397785, 'inboard'),\n",
       " (0.24889594928684794, 'mesoproterozoic'),\n",
       " (0.24687598025117824, 'outboard'),\n",
       " (0.2278460611143454, 'basinal'),\n",
       " (0.2272224309502494, 'merriam'),\n",
       " (0.22362673488291623, 'geophysicists')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = word_ind[2]\n",
    "print(\"Similar terms to:\", features[ind])# Get most similar terms according to the cosine similarity of their vectors (columns in the term-document matrix)\n",
    "heapq.nlargest(10, zip(cosine_similarity(tfidf_matrix[:,ind].todense().T, tfidf_matrix.todense().T)[0], features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar terms to: examinations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1.0, 'examinations'),\n",
       " (0.32351699170687753, 'nicl'),\n",
       " (0.2288021645760516, 'geologist'),\n",
       " (0.22122448951708498, 'impoundment'),\n",
       " (0.21025845052527645, 'patriotism'),\n",
       " (0.20676008495200174, 'exams'),\n",
       " (0.2017877687354704, 'administering'),\n",
       " (0.19248055335809805, 'owns'),\n",
       " (0.1742790072841303, 'trnl'),\n",
       " (0.16707621036056752, 'airshed')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = word_ind[3]\n",
    "print(\"Similar terms to:\", features[ind])# Get most similar terms according to the cosine similarity of their vectors (columns in the term-document matrix)\n",
    "# Get most similar terms according to the cosine similarity of their vectors (columns in the term-document matrix)\n",
    "heapq.nlargest(10, zip(cosine_similarity(tfidf_matrix[:,ind].todense().T, tfidf_matrix.todense().T)[0], features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar terms to: formulations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.9999999999999998, 'formulations'),\n",
       " (0.3340480097056673, 'bedload'),\n",
       " (0.19014132395297081, 'fruits'),\n",
       " (0.1882831748273899, 'gravel'),\n",
       " (0.17897257778342177, 'parker'),\n",
       " (0.16935091394652738, 'harvest'),\n",
       " (0.16892021909460597, 'anthony'),\n",
       " (0.16429684939256983, 'comprehensively'),\n",
       " (0.16260114322707842, 'farms'),\n",
       " (0.1598158183782366, 'sensitively')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = word_ind[4]\n",
    "print(\"Similar terms to:\", features[ind])# Get most similar terms according to the cosine similarity of their vectors (columns in the term-document matrix)\n",
    "# Get most similar terms according to the cosine similarity of their vectors (columns in the term-document matrix)\n",
    "heapq.nlargest(10, zip(cosine_similarity(tfidf_matrix[:,ind].todense().T, tfidf_matrix.todense().T)[0], features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment 1\n",
    "This seems to work quite well, some work better than others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim==3.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/6e/8a8ff9ec36a34dd753c6504cde998c4e0a4e37dcd91e1c9ca4b71960a4f5/gensim-3.6.0.tar.gz (23.1MB)\n",
      "\u001b[K     |████████████████████████████████| 23.2MB 10.4MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: gensim\n",
      "  Building wheel for gensim (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gensim: filename=gensim-3.6.0-cp37-cp37m-linux_x86_64.whl size=24238816 sha256=6af87241bf92e2f86dd7cde54c98c37201f0d89da3ece239c7da19fa47185328\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/97/37/bd/2b80cbc3be93cdf9dd3348dc8b5e659e059f5b51fcecc706be\n",
      "Successfully built gensim\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-3.6.0\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-deps gensim==3.6.0 #for some reason ginsem had trouble installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train word vectors\n",
    "\n",
    "import gensim # Make sure you also have cython installed to accelerate computation!\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Train word2vec model\n",
    "vectors = gensim.models.Word2Vec(tokenized_text, size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(vectors.wv.vocab.keys())\n",
    "words = [vocab[random.randint(0, len(vocab))] for x in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar terms to: barometry\n",
      "[('39', 0.9313677549362183), ('Features', 0.9276204109191895), ('Be', 0.9201106429100037), ('Pr', 0.9197953343391418), ('1300', 0.9195294976234436), ('57', 0.9187619686126709), ('interbedded', 0.9157347679138184), ('Cd', 0.9157118797302246), ('foraminifer', 0.9132989645004272), ('labelled', 0.9082990884780884)]\n",
      "\n",
      "Similar terms to: us\n",
      "[('them', 0.7019619941711426), ('we', 0.6453989148139954), ('ability', 0.6231895685195923), ('attempts', 0.6215698719024658), ('better', 0.6208797097206116), ('insight', 0.6116805076599121), ('attempt', 0.6029911041259766), ('try', 0.5978108644485474), ('help', 0.5975576043128967), ('predictions', 0.593244194984436)]\n",
      "\n",
      "Similar terms to: mounted\n",
      "[('cable', 0.8753342628479004), ('wafer', 0.8694616556167603), ('tunable', 0.8600301742553711), ('THz', 0.8567683696746826), ('Transmission', 0.8559300899505615), ('miniature', 0.8531812429428101), ('pulsed', 0.849880039691925), ('SiGe', 0.8416298627853394), ('resonator', 0.8402810096740723), ('sapphire', 0.8397958278656006)]\n",
      "\n",
      "Similar terms to: separable\n",
      "[('Monge', 0.906668484210968), ('Yau', 0.9031857252120972), ('Sobolev', 0.9027457237243652), ('Kac', 0.9024497270584106), ('Minkowski', 0.8985624313354492), ('Calabi', 0.8962969779968262), ('equivariant', 0.8955736756324768), ('toric', 0.8951283097267151), ('semigroups', 0.8910166025161743), ('Toeplitz', 0.8894329071044922)]\n",
      "\n",
      "Similar terms to: supervised\n",
      "[('advised', 0.7812854647636414), ('presenting', 0.776923656463623), ('summarizing', 0.7752585411071777), ('supplemented', 0.7744811177253723), ('mentored', 0.7734332084655762), ('reviewed', 0.7698084115982056), ('Otter', 0.7676863074302673), ('2006', 0.7645817995071411), ('steering', 0.7643778324127197), ('afforded', 0.75688636302948)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect words with vectors most similar to a given word\n",
    "for word in words:\n",
    "    print(\"Similar terms to:\", word)\n",
    "    print(vectors.wv.most_similar(word))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment\n",
    "\n",
    "Again, ginsem stopped working so I was not able to play around with the settings as much. The selection of words were randomised and are different from the ones used previously. It seems to work relatively well (eg: supervised is similar to advised, mentored etc) and for some reason separable was considered similar to names associated with mathematics (Monge cone, Sobolev space, Shing-Tung Yau, etc - all of which are associated with differential equations), which makes sense on an abstract level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_elmo(sents):\n",
    "    embeddings = elmo(sents, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        return sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/elmo/3\"\n",
    "embed = hub.KerasLayer(module_url)\n",
    "def elmo_vectors(sents):\n",
    "    embeddings = embed(tf.constant(sents))\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        sess.run(tf.compat.v1.global_variables_initializer(), tf.compat.v1.disable_eager_execution())\n",
    "        return sess.run(embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = \"\"\"Her favorite fruit to eat is a date\n",
    "Joe took Alexandria out on a date\n",
    "Not to date myself, but I remember listening to radio shows as a kid\n",
    "What is your date of birth\"\"\".lower().split('\\n')\n",
    "\n",
    "sentences = []\n",
    "for s in sents:\n",
    "    sentences.append(s.split())\n",
    "target = 'date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING:tensorflow:Entity <bound method KerasLayer.call of <tensorflow_hub.keras_layer.KerasLayer object at 0x7f5c6db7b518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method KerasLayer.call of <tensorflow_hub.keras_layer.KerasLayer object at 0x7f5c6db7b518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method KerasLayer.call of <tensorflow_hub.keras_layer.KerasLayer object at 0x7f5c6db7b518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "Sentence: her favorite fruit to eat is a date\n",
      "Vector for 'date': 0.007929273\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method KerasLayer.call of <tensorflow_hub.keras_layer.KerasLayer object at 0x7f5c6db7b518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method KerasLayer.call of <tensorflow_hub.keras_layer.KerasLayer object at 0x7f5c6db7b518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method KerasLayer.call of <tensorflow_hub.keras_layer.KerasLayer object at 0x7f5c6db7b518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "Sentence: joe took alexandria out on a date\n",
      "Vector for 'date': 0.8145567\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method KerasLayer.call of <tensorflow_hub.keras_layer.KerasLayer object at 0x7f5c6db7b518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method KerasLayer.call of <tensorflow_hub.keras_layer.KerasLayer object at 0x7f5c6db7b518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method KerasLayer.call of <tensorflow_hub.keras_layer.KerasLayer object at 0x7f5c6db7b518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "Sentence: not to date myself, but i remember listening to radio shows as a kid\n",
      "Vector for 'date': 0.53625405\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method KerasLayer.call of <tensorflow_hub.keras_layer.KerasLayer object at 0x7f5c6db7b518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method KerasLayer.call of <tensorflow_hub.keras_layer.KerasLayer object at 0x7f5c6db7b518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method KerasLayer.call of <tensorflow_hub.keras_layer.KerasLayer object at 0x7f5c6db7b518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "Sentence: what is your date of birth\n",
      "Vector for 'date': -0.25847456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#elmo_vecs = elmo_vectors(sents)\n",
    "word_vecs = []\n",
    "print()\n",
    "for i, sent in enumerate(sents):\n",
    "    elmo_vecs = elmo_vectors(sent.split())\n",
    "    word_vecs.append(elmo_vecs[i][sent.split().index(target)])\n",
    "    print(\"Sentence:\", sent)\n",
    "    print(\"Vector for '%s':\" % target, word_vecs[-1])\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment:\n",
    "\n",
    "Here I will present the results in a more easily readable format:\n",
    "\n",
    "Sentence: her favorite fruit to eat is a date\n",
    "Vector for 'date': 0.007929273\n",
    "\n",
    "Sentence: joe took alexandria out on a date\n",
    "Vector for 'date': 0.8145567 \n",
    "    \n",
    "Sentence: not to date myself, but i remember listening to radio shows as a kid\n",
    "Vector for 'date': 0.53625405\n",
    "     \n",
    "Sentence: what is your date of birth\n",
    "Vector for 'date': -0.25847456\n",
    "\n",
    "\n",
    "This seems to work quite well although I can't currently comment any further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
