{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I have not had any collaborators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_lines\n",
    "import numpy \n",
    "import os \n",
    "import re \n",
    "import binascii \n",
    "from time import time\n",
    "from os import listdir\n",
    "import itertools\n",
    "import tarfile\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Read tweets in python__\n",
    "\n",
    "__2. Extract the actual text fields from the tweet jsons, discard all metadata at this point.               Note that sometimes the text may be truncated to fit the old character limit. In these                cases, is it possible to get the full text?__\n",
    "    Yes, this has been done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    '''\n",
    "    Extract tweets (truncated and nontruncated), \n",
    "    lowercase everything, \n",
    "    remove link to tweets at the end \n",
    "    '''\n",
    "    data = []\n",
    "    with open('english-tw\n",
    "              eets-sample.jsonl', 'rb') as f:\n",
    "        for item in json_lines.reader(f):            \n",
    "            # if truncated, you can access the extended tweets\n",
    "            if item['truncated']==True:\n",
    "                data.append(item['extended_tweet']['full_text'].lower().split('https:')[0]) \n",
    "            else:\n",
    "                data.append(item['text'].lower().split('https:')[0])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Segment each tweet using both UDPipe machine learned model (can be found from             the same data package) and a heuristic method. What can you tell about the              segmentation performance on tweets when manually inspecting few examples?__ \n",
    "\n",
    "__Note: UDPipe approach kept restarting the kernel and would not work for me so I decided to use the nltk.tonkenize.TweetTokenizer approach.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%?` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['check', 'out', 'my', 'class', 'in', '#granbluefantasy', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "result= tknzr.tokenize(data[0])\n",
    "print(result)\n",
    "data_tok = [tknzr.tokenize(d) for d in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments on tokenization:\n",
    "It seems to work quite well. It even tokenizes the emojis. Unfortunately it does not handle contractions. Again, I would have used UDPipe.pipeline if it didn't constantly restart my notebook kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = Counter()\n",
    "for tweet in data:\n",
    "    tokenized = tknzr.tokenize(tweet)\n",
    "    token_counter.update(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. Count a word frequency list (how many times each word appears and how many              unique words there are). Which are the most common words appearing in the data?              What kind of words these are?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 54\n",
      "Tokens: [('rt', 5801), ('christmas', 517), ('Ô∏è', 481), ('like', 454), ('üòÇ', 447), ('one', 380), ('love', 361), ('new', 347), ('people', 335), ('get', 309), ('day', 291), ('year', 275), ('2017', 269), ('good', 245), ('time', 241), ('today', 240), ('1', 218), ('see', 216), ('know', 203), ('‚ù§', 202), ('u', 199), ('want', 199), ('best', 193), ('need', 192), (\"i'm\", 190), ('2018', 190), ('happy', 190), ('got', 186), ('family', 186), ('back', 183), ('make', 178), ('go', 171), ('follow', 170), ('life', 167), ('much', 165), ('thank', 163), ('merry', 161), ('first', 153), ('üéÑ', 153), ('2', 152), ('great', 152), ('really', 151), ('us', 151), ('..', 150), ('right', 143), ('would', 143), ('üòç', 143), ('think', 141), ('even', 141), ('$', 139), ('everyone', 138), ('trump', 136), ('ever', 133), ('üò≠', 131)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\edvar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords') # download the stopwords dataset\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "filtered_tokens = []\n",
    "\n",
    "# list of punctuation symbols to ignore\n",
    "punctuation_chars = '. , : ( ) ! ? \" = & - ; ... \\\\ ‚Äú ‚Ä¶ * ‚Äô / ‚Äù' .split() \n",
    "punctuation_chars+=\"'\"\n",
    "\n",
    "for word, count in token_counter.most_common(150):\n",
    "    if word.lower() in stopwords.words(\"english\") or word in punctuation_chars:\n",
    "        continue\n",
    "    filtered_tokens.append((word, count))\n",
    "print(\"Number of tokens:\", len(filtered_tokens))\n",
    "print(\"Tokens:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments on word frequency\n",
    "It appears that the tweets were collected in the month of december due to the frequent use of 'chrismas','merry','new', 'year' etc. It seems that many of the tweets are retweets, if that's what RT refers to. Which is the case as there are 1270 douplicates in the copus.\n",
    "\n",
    "As I used 'nlkt.TweetTokenizer' we also have the emojis used. In this case there seem to also be a number of christmas related emojis present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Number of duplicates: 1270'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Number of duplicates: {len(data)-len(set(data))}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'26212 tokens in the data'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{len(token_counter)} tokens in the data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. Calculate ‚Äãidf weight for each word appearing in the data (one tweet = one              document), and print top 20 words with lowest and highest idf values. Why ‚Äãtf does               not really matter when processing tweets?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "for word, count in filtered_tokens:\n",
    "    if word.lower() in stopwords.words(\"english\") or word in punctuation_chars:\n",
    "        continue\n",
    "    word_dict[word] = 0\n",
    "    #use the tokenized data in order to accurately count\n",
    "    # i.e. tokenized tweets \n",
    "    for d in data_tok: \n",
    "        if word.lower() in d:\n",
    "            word_dict[word]+=1\n",
    "            #word_df.update(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf = pd.DataFrame()\n",
    "word_list, idf = [], [] \n",
    "for word in word_dict.keys():\n",
    "    word_list.append(word)\n",
    "    idf.append(len(data)/word_dict[word])\n",
    "df_idf['word'] = word_list\n",
    "df_idf['idf'] = idf\n",
    "df_idf = df_idf.sort_values('idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['christmas', 'like', 'one', 'love', 'new', 'people', 'get', 'day',\n",
       "       'year', '2017', 'today', 'good', 'time', 'see', 'know', 'want',\n",
       "       'need', 'got', 'best', 'happy'], dtype=object)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf.head(20).word.values #20 Lowest IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['layout', 'whyyyyy', '1780', 'byeeeeeeeeeeeeeeeeeeeeee', 'moritz',\n",
       "       'hatchet', '@shreya1526', 'sidharth', 'malhotra', '@regalglobal',\n",
       "       'maserati', 'bazalwane', '@gamergod240', 'brockhampton', 'abysmal',\n",
       "       '@brainpicker', 'scientist', 'coined', 'launched', '@swo22'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf.tail(20).word.values #20 Highest idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on IDF\n",
    "\n",
    "It seems that Christmas related words were given the lowest IDF, probably due to their high prevalence during the months close to december which increasingly seems like the time of year these tweets were sampled.\n",
    "\n",
    "The highest IDFs were observed for pseudowords, words that are not particularly prevalent and some profile taggers.\n",
    "\n",
    "Term Frequency (TF) would be almost pointless to calculate as tweets are so short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6 ‚ÄãFind duplicate or           near duplicate tweets (in terms of text field only) in the data using any method you                see fit. What kind of techniques you considered using and/or tested, and how many              duplicate or near duplicate did you find?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global parameters to process the whole dataset\n",
    "bands = 5\n",
    "rows = 5\n",
    "nsig = bands*rows  # number of elements in signature, or the number of different random hash functions\n",
    "\n",
    "maxShingleID = 2**32-1  # record the maximum shingle ID that we assigned\n",
    "nextPrime = 4294967311  # next prime number after maxShingleID\n",
    "\n",
    "A = numpy.random.randint(0, nextPrime, size=(nsig,), dtype='int64')\n",
    "B = numpy.random.randint(0, nextPrime, size=(nsig,), dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shingles(text, k=5):\n",
    "    \"\"\"Get all shingles from requested file (hashes of these shingles)\n",
    "    \"\"\"\n",
    "    L = len(text)\n",
    "    shingles = set()  # we use a set to automatically eliminate duplicates\n",
    "    for i in range(L-k+1):\n",
    "        shingle = text[i:i+k]\n",
    "        crc = binascii.crc32(shingle.encode('utf-8')) #& 0xffffffff  # hash the shingle to a 32-bit integer\n",
    "        shingles.add(crc)\n",
    "    return shingles\n",
    "\n",
    "def jaccard_similarity_score(x, y):\n",
    "    \"\"\"\n",
    "    Jaccard Similarity J (A,B) = | Intersection (A,B) | /\n",
    "                                    | Union (A,B) |\n",
    "    \"\"\"\n",
    "    intersection_cardinality = len(set(x).intersection(set(y)))\n",
    "    union_cardinality = len(set(x).union(set(y)))\n",
    "    return intersection_cardinality / float(union_cardinality)\n",
    "\n",
    "#jaccard_similarity_score(shingles_vectors[0], shingles_vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast implementation of Minhash algorithm\n",
    "# computes all random hash functions for a shingle at once, using vector operations\n",
    "# also finds element-wise minimum of two vectors efficiently\n",
    "def minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig):\n",
    "    signature = numpy.ones((nsig,)) * (maxShingleID + 1)\n",
    "\n",
    "    for ShingleID in shingles:\n",
    "        hashCodes = ((A*ShingleID + B) % nextPrime) % maxShingleID\n",
    "        numpy.minimum(signature, hashCodes, out=signature)\n",
    "\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSH(signatures, bands, rows, Ab, Bb, nextPrime, maxShingleID):\n",
    "    \"\"\"Locality Sensitive Hashing\n",
    "    \"\"\"\n",
    "    numItems = signatures.shape[1]\n",
    "    signBands = numpy.array_split(signatures, bands, axis=0)\n",
    "    candidates = set()\n",
    "    for nb in range(bands):\n",
    "        hashTable = {}\n",
    "        for ni in range(numItems):\n",
    "            item = signBands[nb][:,ni]\n",
    "            hash = (numpy.dot(Ab[nb,:], item) + Bb[nb]) % nextPrime % maxShingleID\n",
    "            if hash not in hashTable:\n",
    "                hashTable[hash] = [ni]\n",
    "            else:\n",
    "                hashTable[hash].append(ni)\n",
    "        for _,items in hashTable.items():\n",
    "            if len(items) > 1:\n",
    "                L = len(items)\n",
    "                for i in range(L-1):\n",
    "                    for j in range(i+1, L):\n",
    "                        cand = [items[i], items[j]]\n",
    "                        numpy.sort(cand)\n",
    "                        candidates.add(tuple(cand))\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LSH(fnames, s=.9, k=5):\n",
    "\n",
    "    # find candidates with LSH\n",
    "\n",
    "    signatures = []  # signatures for all files\n",
    "    for fname in fnames:\n",
    "        shingles = get_shingles(fname, k=k)\n",
    "        signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "        signatures.append(signature)\n",
    "\n",
    "    # prepare data for LSH\n",
    "    A2 = numpy.random.randint(0, nextPrime/2, size=(bands, rows), dtype='int64')  # now we need a vector of A parameters for each band\n",
    "    B2 = numpy.random.randint(0, nextPrime/2, size=(bands, ), dtype='int64')\n",
    "    signatures = numpy.array(signatures).T  # LSH needs a matrix of signatures, not a list of vectors\n",
    "\n",
    "    Nfiles = signatures.shape[1]  # number of different files\n",
    "    t = time()\n",
    "    candidates = LSH(signatures, bands, rows, A2, B2, nextPrime, maxShingleID)\n",
    "    t2 = time() - t\n",
    "    \n",
    "    return candidates, t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates, _ = get_LSH(data, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5721"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5721 candidates here because many are retweets of the same tweet. As shown earlier there are 1270 duplicates in the data, probably due to several people retweeting similar tweets and automated messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1270"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)-len(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rt @slushiimusic: mic drop @bts_twt ', 30),\n",
       " ('your overall health is on your mind today as you anticipate th... more for scorpio ',\n",
       "  24),\n",
       " ('rt @louis_tomlinson: thank you so much for all the birthday messages and i hope everyone had a great christmas ! loads of love',\n",
       "  22),\n",
       " ('i liked a @youtube video ', 22),\n",
       " ('an unfinished discussion with a family member may unexpectedly... more for virgo ',\n",
       "  20)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(data).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use set() to remove the duplicates, or use them.. you could simply use Counter() and find all of the duplicates.\n",
    "\n",
    "The most common tweet is a tweet that has been retweeted 30 times. So many duplicates will consist of several people retweeting the same thing.\n",
    "\n",
    "Some seem like they are automated, for example the 'i liked a @youtube video', probably linked to their youtube account etc. \n",
    "\n",
    "Then there are some that are related to horoscopes (i.e. one references virgo and the other scorpio). Again, automated messages.\n",
    "\n",
    "Someone might be interested in duplicates, _I have chosen to ignore them so that I can more sanely compare the performance of the Local Sensitivity Hashing that I have deployed here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set = list(set(data))\n",
    "candidates_set, _ = get_LSH(data_set, k=5)\n",
    "len(candidates_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pair(n=42):\n",
    "    print(f'Candidate pair #{n}:')\n",
    "    print(data_set[list(candidates_set)[n][0]], data_set[list(candidates_set)[n][1]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate pair #2:\n",
      "#videomtv2017 louis tomlinson üéÇ #videomtv2017 louis tomlinson üëó\n",
      "\n",
      "Candidate pair #33:\n",
      "@5hvotingstats b #videomtv2017 fifth harmony @5hvotingstats ff #videomtv2017 fifth harmony\n",
      "\n",
      "Candidate pair #42:\n",
      "2 people unfollowed me // automatically checked by  4 people followed me and 4 people unfollowed me // automatically checked by \n",
      "\n",
      "Candidate pair #15:\n",
      "rt @football__tweet: harry kane has now scored as many hat-tricks (6) as every other player in the premier league combined in 2017.\n",
      "\n",
      "that i‚Ä¶ rt @amaradatia: harry kane has now scored as many hat-tricks (6) as every other player in the premier league combined in 2017.\n",
      "\n",
      "that is a m‚Ä¶\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None, None)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_pair(2), print_pair(33) ,print_pair(), print_pair(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "194 candidate pairs were flagged.\n",
    "\n",
    "As we can see, the algorithm identified similar hashtags with different emojis. The '#videomtv2017 louis tomlinson' is extremely prevalent in this dataset. It seems that this person had their birthday in that month and is/was a popular person on twitter.\n",
    "\n",
    "It sill identified automated messages, for example '_N_ number of people __unfollowed me__' and '_N_ number of people __followed me__', which are similar.\n",
    "\n",
    "Candidate pair 15 found a tweet with a retweet and a subsequent comment.\n",
    "\n",
    "I chose to use LSH hashing with Jaccard Similarity (I could have also used Cosine Distance) as it is fairly simple and fast. I could have also used minHashing or pairwise comparison. I ruled out pairwaise comparison simply because it is so slow, but it would have reduced false positives and false negatives.\n",
    "\n",
    "##### Summation\n",
    "\n",
    "One could filter through the data and ignore retweets as they will obviously be flagged by LSH, but it really depends on what one is trying to do. This method seems to work fairly well and is relatively fast.\n",
    "\n",
    "One could also choose to ignore the '#....' and/or '@....' in order to  analyse the text content only. Otherwise, the hashtags and tags can and do influence this pipeline. Again, it really depends on what the purpose is. I have included them here to keep things simple.\n",
    "\n",
    "You could also filter out the automated messages if you had a list of automated messages that are generated in twitter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
